{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect sarcasm in news headlines with TensorFlow\n",
    "This work is part of a collection of practice sets called [NLP Starter](https://github.com/jamiemorales/project-nlp-starter).\n",
    "It aims to help someone get started fast and gain a high-level understanding of the fundamental steps in the NLP lifecycle early on.\n",
    "After completion, someone will have built intuition over the NLP lifecycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Understand the problem\n",
    "What we're trying to do here is to classify whether a news headline is sarcastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set-up and understand data\n",
    "In this step, we layout the tools we will need to solve the problem identified in the previous step. We want to inspect our data sources and explore the data itself to gain an understanding of the data for preprocessing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Set-up libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_json('../00-Datasets/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26709 entries, 0 to 26708\n",
      "Data columns (total 3 columns):\n",
      "article_link    26709 non-null object\n",
      "headline        26709 non-null object\n",
      "is_sarcastic    26709 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 626.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Look at some details\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14985\n",
       "1    11724\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at breakdown of label\n",
    "df['is_sarcastic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data and understand some more\n",
    "In this step, we perform the necessary transformations on the data so that the neural network would be able to understand it. Real-world datasets are complex and messy. For our purposes, most of the datasets we work on in this series require minimal preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21367,)\n",
      "(5342,)\n",
      "(21367,)\n",
      "(5342,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 80% training and 20% validation\n",
    "sentences = df['headline']\n",
    "labels = df['is_sarcastic']\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "print(train_sentences.shape)\n",
    "print(val_sentences.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad\n",
    "vocab_size = 10000\n",
    "oov_token = '<00V>'\n",
    "max_length = 100\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build, train, and evaluate neural network\n",
    "First, we design the neural network, e.g., sequence of layers and activation functions. \n",
    "\n",
    "Second, we train the neural network, we iteratively make a guess, calculate how accurate that guess is, and enhance our guess. The first guess is initialised with random values. The goodness or badness of the guess is measured with the loss function. The next guess is generated and enhanced by the optimizer function.\n",
    "\n",
    "Lastly, we apply use the neural network on previously unseen data and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21367 samples\n",
      "Epoch 1/10\n",
      "21367/21367 - 1s - loss: 0.6835 - accuracy: 0.5606\n",
      "Epoch 2/10\n",
      "21367/21367 - 1s - loss: 0.6360 - accuracy: 0.6381\n",
      "Epoch 3/10\n",
      "21367/21367 - 1s - loss: 0.4622 - accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "21367/21367 - 1s - loss: 0.3441 - accuracy: 0.8710\n",
      "Epoch 5/10\n",
      "21367/21367 - 1s - loss: 0.2912 - accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "21367/21367 - 1s - loss: 0.2592 - accuracy: 0.9012\n",
      "Epoch 7/10\n",
      "21367/21367 - 1s - loss: 0.2331 - accuracy: 0.9137\n",
      "Epoch 8/10\n",
      "21367/21367 - 1s - loss: 0.2134 - accuracy: 0.9222\n",
      "Epoch 9/10\n",
      "21367/21367 - 1s - loss: 0.1961 - accuracy: 0.9294\n",
      "Epoch 10/10\n",
      "21367/21367 - 1s - loss: 0.1809 - accuracy: 0.9356\n"
     ]
    }
   ],
   "source": [
    "# Build and train neural network\n",
    "embedding_dim = 16\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "history = model.fit(train_padded, train_labels, batch_size=batch_size, epochs=num_epochs, \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5342/5342 [==============================] - 0s 38us/sample - loss: 0.3366 - accuracy: 0.8575\n",
      "Val loss: 0.3366185913747368, Val accuracy: 85.75440049171448\n",
      "\n",
      "canada is flattening the coronavirus curve\n",
      "Unlikely sarcasm. Sarcasm score: [0.6456852]\n",
      "\n",
      "canucks take home the cup\n",
      "Unlikely sarcasm. Sarcasm score: [7.4036417]\n",
      "\n",
      "safety meeting ends in accident\n",
      "Possible sarcasm. Sarcasm score: [56.476707]\n"
     ]
    }
   ],
   "source": [
    "# Apply neural network\n",
    "val_loss, val_accuracy = model.evaluate(val_padded, val_labels)\n",
    "print('Val loss: {}, Val accuracy: {}'.format(val_loss, val_accuracy*100))\n",
    "\n",
    "quick_test_sentence = [\n",
    "    'canada is flattening the coronavirus curve',\n",
    "    'canucks take home the cup',\n",
    "    'safety meeting ends in accident'\n",
    "    \n",
    "]\n",
    "\n",
    "quick_test_sequences = tokenizer.texts_to_sequences(quick_test_sentence)\n",
    "quick_test_padded = pad_sequences(quick_test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "quick_test_sentiments = model.predict(quick_test_padded)\n",
    "\n",
    "for i in range(len(quick_test_sentiments)):\n",
    "    print('\\n' + quick_test_sentence[i])\n",
    "    if 0 < quick_test_sentiments[i] < .50:\n",
    "        print('Unlikely sarcasm. Sarcasm score: {}'.format(quick_test_sentiments[i]*100))\n",
    "    elif .50 < quick_test_sentiments[i] < .75:\n",
    "        print('Possible sarcasm. Sarcasm score: {}'.format(quick_test_sentiments[i]*100))\n",
    "    elif .75 >  quick_test_sentiments[i] < 1:\n",
    "        print('Sarcasm. Sarcasm score:  {}'.format(quick_test_sentiments[i]*100))\n",
    "    else:\n",
    "        print('Not in range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "If you found this work interesting, you might like:\n",
    "\n",
    "* Machine Learning Starter\n",
    "\n",
    "* Deep Learning Starter\n",
    "\n",
    "* Natural Language Processing Starter\n",
    "\n",
    "You can find more at [github.com/jamiemorales](https://github.com/jamiemorales).\n",
    "\n",
    "Datasets are not mine. List of sources: [datasets and sources]()\n",
    "\n",
    "For sharing this work, here's how / the license: https://creativecommons.org/licenses/by-sa/4.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
